import pandas as pd

data = pd.read_csv('Ð¸Ñ‚Ð¾Ð³2.csv')

def json_to_dataframe_simple(json_file):
    df = pd.read_json(json_file)
    return df

data2 = json_to_dataframe_simple('response.json')
data2=data2[:50]
result = pd.concat([data, data2],  axis=1)

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, ExtraTreesRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import HistGradientBoostingRegressor
import warnings
warnings.filterwarnings('ignore')

def calculate_competition_metrics(y_true, y_pred, probabilities=None, base_mae=None, base_brier=None):
    metrics = {}
    
    # 1. MAE (Mean Absolute Error)
    mae = mean_absolute_error(y_true, y_pred)
    metrics['MAE'] = mae
    
    # ÐÐ¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ MAE (Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð±ÐµÐ¹Ð·Ð»Ð°Ð¹Ð½Ð°)
    if base_mae is not None and base_mae > 0:
        mae_norm = max(0, 1 - (mae / base_mae))
        metrics['MAE_norm'] = mae_norm
    else:
        metrics['MAE_norm'] = 0.0
    
    # 2. Brier Score (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸)
    if probabilities is not None:
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð±Ð¸Ð½Ð°Ñ€Ð½Ñ‹Ðµ Ð¼ÐµÑ‚ÐºÐ¸: 1 ÐµÑÐ»Ð¸ Ñ€Ð¾ÑÑ‚, 0 ÐµÑÐ»Ð¸ Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ
        true_directions = (y_true > 0).astype(int)
        brier = np.mean((true_directions - probabilities) ** 2)
        metrics['Brier'] = brier
        
        # ÐÐ¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Brier
        if base_brier is not None and base_brier > 0:
            brier_norm = max(0, 1 - (brier / base_brier))
            metrics['Brier_norm'] = brier_norm
        else:
            metrics['Brier_norm'] = 0.0
    else:
        metrics['Brier'] = None
        metrics['Brier_norm'] = 0.0
    
    # 3. Directional Accuracy (DA)
    true_sign = np.sign(y_true)
    pred_sign = np.sign(y_pred)
    da = np.mean(true_sign == pred_sign)
    metrics['DA'] = da
    
    # 4. Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Score
    score_components = []
    score_components.append(0.7 * metrics['MAE_norm'])
    score_components.append(0.3 * metrics['Brier_norm'])
    score_components.append(0.1 * metrics['DA'])
    
    metrics['Final_Score'] = sum(score_components)
    
    return metrics #70% MAE + 30% Brier + 10% DA

# Ñ€Ð°ÑÑ‡ÐµÑ‚ Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ð¸
def calculate_returns(prices, horizon=1):
    future_prices = prices.shift(-horizon)
    returns = (future_prices / prices) - 1
    return returns

def evaluate_all_targets(test_df, predictions):
    print("\n=== ÐžÐ¦Ð•ÐÐšÐ ÐŸÐž Ð’Ð¡Ð•Ðœ Ð¦Ð•Ð›Ð•Ð’Ð«Ðœ ÐŸÐ•Ð Ð•ÐœÐ•ÐÐÐ«Ðœ ===")
    
    results = {}
    for target in ['open', 'high', 'low', 'close', 'volume']:
        if target in predictions and target in test_df.columns:
            y_true = test_df[target].values
            y_pred = predictions[target]
            
            min_len = min(len(y_true), len(y_pred))
            y_true = y_true[:min_len]
            y_pred = y_pred[:min_len]
            
            mae = mean_absolute_error(y_true, y_pred)
            rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
            
            # MAPE Ñ Ð·Ð°Ñ‰Ð¸Ñ‚Ð¾Ð¹ Ð¾Ñ‚ Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð½Ð° Ð½Ð¾Ð»ÑŒ
            with np.errstate(divide='ignore', invalid='ignore'):
                mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true != 0, y_true, 1))) * 100
            
            results[target] = {
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            }
            
            print(f"\n{target.upper()}:")
            print(f"  MAE: {mae:.4f}")
            print(f"  RMSE: {rmse:.4f}")
            print(f"  MAPE: {mape:.2f}%")
    
    return results

def evaluate_ensemble_with_metrics(test_df, predictions, horizon=1):
    print(f"\n=== ÐžÐ¦Ð•ÐÐšÐ ÐÐÐ¡ÐÐœÐ‘Ð›Ð¯ ÐÐ Ð“ÐžÐ Ð˜Ð—ÐžÐÐ¢Ð• {horizon} Ð”ÐÐ•Ð™ ===")
    
    true_prices = test_df['close'].values
    true_returns = calculate_returns(pd.Series(true_prices), horizon=horizon)
    valid_indices = ~np.isnan(true_returns)
    true_returns = true_returns[valid_indices]
    predicted_prices = predictions['close']
    predicted_prices = predicted_prices[:len(true_returns)]
    current_prices = true_prices[:len(predicted_prices)]
    predicted_returns = (predicted_prices / current_prices) - 1
    bias = np.mean(true_returns) - np.mean(predicted_returns)
    predicted_returns_corrected = predicted_returns + bias
    
    # Ð”Ð»Ñ Brier score - Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸
    min_ret, max_ret = predicted_returns_corrected.min(), predicted_returns_corrected.max()
    if max_ret > min_ret:
        normalized_returns = (predicted_returns_corrected - min_ret) / (max_ret - min_ret)
    else:
        normalized_returns = np.full_like(predicted_returns_corrected, 0.5)
    
    probabilities = normalized_returns
    
    # Ð‘ÐµÐ¹Ð·Ð»Ð°Ð¹Ð½Ñ‹
    base_mae = np.mean(np.abs(true_returns - np.mean(true_returns)))
    base_brier = 0.25
    
    # Ð Ð°ÑÑ‡ÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
    metrics = calculate_competition_metrics(
        y_true=true_returns,
        y_pred=predicted_returns_corrected,
        probabilities=probabilities,
        base_mae=base_mae,
        base_brier=base_brier
    )
    
    return metrics, predicted_returns_corrected, true_returns

# Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
def create_advanced_momentum_features(df):
    df = df.copy()
    
    # ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ñ†ÐµÐ½Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
    if 'close' in df.columns:
        # Ð Ð°Ð·Ð½Ð¾ÑÑ‚Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð²Ð¼ÐµÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð½Ñ‹Ñ… Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹ 
        for period in [1, 2, 3]:
            df[f'price_diff_{period}'] = df['close'].diff(period)
            df[f'price_change_{period}'] = df['close'].pct_change(period)
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ
        rolling_std = df['close'].pct_change().rolling(10).std()
        df['normalized_change'] = df['close'].pct_change() / (rolling_std + 1e-8)
        
        # Ð¢Ñ€ÐµÐ½Ð´Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        for window in [3, 5, 8]:
            sma = df['close'].rolling(window).mean()
            df[f'trend_{window}'] = (df['close'] - sma) / sma
            df[f'momentum_{window}'] = df['close'] / df['close'].shift(window) - 1
            
            # Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ñ‚Ñ€ÐµÐ½Ð´Ð°
            if window > 3:
                df[f'acceleration_{window}'] = df[f'trend_{window}'] - df[f'trend_{window}'].shift(1)
    
    # ÐžÐ±ÑŠÐµÐ¼Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    if 'volume' in df.columns:
        volume_sma = df['volume'].rolling(10).mean()
        volume_std = df['volume'].rolling(10).std()
        df['volume_ratio'] = df['volume'] / (volume_sma + 1e-8)
        df['volume_zscore'] = (df['volume'] - volume_sma) / (volume_std + 1e-8)
        
        # Ð’Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ Ð¾Ð±ÑŠÐµÐ¼Ð° Ð¸ Ñ†ÐµÐ½Ñ‹
        if 'close' in df.columns:
            price_change = df['close'].pct_change()
            df['volume_price_corr'] = price_change.rolling(5).corr(df['volume'].pct_change())
    
    # Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹ Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    if 'rsi' in df.columns:
        df['rsi_normalized'] = (df['rsi'] - 50) / 30  # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ð¾ÐºÑ€ÑƒÐ³ 50
        df['rsi_signal'] = np.where(df['rsi'] > 65, 1, np.where(df['rsi'] < 35, -1, 0))
    
    if 'macd' in df.columns:
        macd_std = df['macd'].rolling(20).std()
        df['macd_normalized'] = df['macd'] / (macd_std + 1e-8)
        df['macd_signal'] = np.sign(df['macd'])
    
    # ÐÐ¾Ð²Ð¾ÑÑ‚Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    news_cols = [col for col in df.columns if 'news' in col or 'sentiment' in col]
    for col in news_cols:
        if df[col].dtype in [np.int64, np.float64]:
            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
            col_mean = df[col].rolling(10).mean()
            col_std = df[col].rolling(10).std()
            df[f'{col}_normalized'] = (df[col] - col_mean) / (col_std + 1e-8)
            df[f'{col}_trend'] = df[col] / col_mean - 1
    
    # Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
    if 'begin' in df.columns:
        df['day_of_week_sin'] = np.sin(2 * np.pi * df['begin'].dt.dayofweek / 7)
        df['day_of_week_cos'] = np.cos(2 * np.pi * df['begin'].dt.dayofweek / 7)
        df['month_sin'] = np.sin(2 * np.pi * df['begin'].dt.month / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['begin'].dt.month / 12)
    
    # Ð’Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð½Ñ‹Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹
    if 'close' in df.columns:
        returns = df['close'].pct_change()
        df['volatility_regime'] = returns.rolling(10).std()
        df['high_volatility'] = (df['volatility_regime'] > df['volatility_regime'].quantile(0.7)).astype(int)
    
    return df

def create_balanced_return_model(X_train, y_train):
    models = [
        ('rf', RandomForestRegressor(
            n_estimators=100, 
            max_depth=7, # Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð³Ð»ÑƒÐ±Ð¸Ð½Ñ‹
            min_samples_split=5, # Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ Ð´Ð»Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ
            min_samples_leaf=2, #Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ Ð² Ð»Ð¸ÑÑ‚ÑŒÑÑ…
            random_state=42
        )),
        ('gb', HistGradientBoostingRegressor(
            max_iter=100, # Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹
            max_depth=4, # Ð¼ÐµÐ»ÐºÐ¸Ðµ Ð´ÐµÑ€ÐµÐ²ÑŒÑ
            learning_rate=0.1, # Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
            random_state=42,
            early_stopping=True, ###
            validation_fraction=0.1
        )),
        ('ridge', Ridge(alpha=0.5, random_state=42)),
        ('et', ExtraTreesRegressor(
            n_estimators=80,
            max_depth=6,
            random_state=42
        ))
    ]
    
    # ÐÐ½ÑÐ°Ð¼Ð±Ð»ÑŒ Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    ensemble = VotingRegressor(estimators=models, weights=[3, 2, 1, 2])
    
    return ensemble


def regularized_return_pipeline(train_df, test_df):
    train_df = create_advanced_momentum_features(train_df)
    test_df = create_advanced_momentum_features(test_df)
    
    train_df = train_df.fillna(method='ffill').fillna(method='bfill').fillna(0)
    test_df = test_df.fillna(method='ffill').fillna(method='bfill').fillna(0)

    if 'close' in train_df.columns:
        future_returns = train_df['close'].shift(-1) / train_df['close'] - 1
        
        # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð²Ñ‹Ð±Ñ€Ð¾ÑÑ‹ (Ð¾Ð±Ñ€ÐµÐ·Ð°ÐµÐ¼ Ð½Ð° 5% Ð¸ 95% ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð»ÑÑ…)
        lower_bound = future_returns.quantile(0.05)
        upper_bound = future_returns.quantile(0.95)
        train_df['target_return'] = future_returns.clip(lower_bound, upper_bound)
    
    # ÐžÑ‚Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
    feature_candidates = []
    
    # ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ð½Ñ‹Ðµ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
    priority_categories = [
        'normalized', 'trend', 'momentum', 'diff', 'ratio', 'zscore',
        'signal', 'volatility', 'acceleration', 'corr'
    ]
    
    for col in train_df.columns:
        if (col not in ['ticker', 'begin', 'open', 'high', 'low', 'close', 'volume', 'target_return'] and
            train_df[col].dtype in [np.int64, np.float64]):
            # ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ Ð´Ð»Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ñ‚Ñ€ÐµÐ½Ð´Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
            if any(keyword in col for keyword in priority_categories):
                feature_candidates.append(col)
    
    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
    other_features = [col for col in train_df.columns 
                     if (col not in ['ticker', 'begin', 'open', 'high', 'low', 'close', 'volume', 'target_return'] + feature_candidates and
                         train_df[col].dtype in [np.int64, np.float64])]
    
    feature_columns = feature_candidates + other_features[:10]  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¾Ð±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾
    
    print(f"ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð½Ð¾ {len(feature_columns)} Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²")
    print(f"Ð›ÑƒÑ‡ÑˆÐ¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸: {feature_columns[:12]}")
    
    # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
    if 'target_return' in train_df.columns:
        train_data = train_df[:-1].copy() 
        X_train = train_data[feature_columns]
        y_train = train_data['target_return']
        
        valid_mask = ~y_train.isnull() & ~X_train.isnull().any(axis=1)
        X_train = X_train[valid_mask]
        y_train = y_train[valid_mask]
        
        if len(X_train) > 8:
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            model = create_balanced_return_model(X_train_scaled, y_train)
            model.fit(X_train_scaled, y_train)
            X_test = test_df[feature_columns]
            X_test_scaled = scaler.transform(X_test)
            raw_predictions = model.predict(X_test_scaled)
            prediction_std = np.std(raw_predictions)
            prediction_mean = np.mean(raw_predictions)
            capped_predictions = np.clip(
                raw_predictions, 
                prediction_mean - 2 * prediction_std,
                prediction_mean + 2 * prediction_std
            )
            
            # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑÐ³Ð»Ð°Ð¶Ð¸Ð²Ð°Ð½Ð¸Ðµ
            smoothed_predictions = 0.7 * capped_predictions + 0.3 * prediction_mean
            
            # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² Ñ†ÐµÐ½Ñ‹
            last_train_price = train_df['close'].iloc[-1]
            predicted_prices = last_train_price * (1 + smoothed_predictions)
            
            return predicted_prices, smoothed_predictions # Ñ†ÐµÐ½Ñ‹, Ð´Ð¾Ñ…Ð¾Ð´Ð½Ð¾ÑÑ‚Ð¸
        else:
            print("ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ")
            return None, None
    else:
        print("ÐÐµÐ²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ñ†ÐµÐ»ÐµÐ²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ")
        return None, None


def optimized_final_pipeline(train_df, test_df):
    return_prices, return_predictions = regularized_return_pipeline(train_df.copy(), test_df.copy())
    
    if return_prices is None:
        last_price = train_df['close'].iloc[-1]
        fallback_predictions = np.full(len(test_df), last_price)
        return_prices = fallback_predictions
        return_predictions = np.zeros(len(test_df))
    
    final_predictions = {}
    final_predictions['close'] = return_prices
    if 'close' in final_predictions:
        recent_train = train_df.tail(10)  
        
        for col in ['open', 'high', 'low']:
            if col in recent_train.columns:
                ratios = recent_train[col] / recent_train['close']
                current_ratio = ratios.mean()
                noise = np.random.normal(0, 0.001, len(return_prices))
                final_predictions[col] = return_prices * (current_ratio + noise)
                
                print(f"ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ {col} (ratio {current_ratio:.4f})")
        
        if 'volume' in train_df.columns:
            # ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: ÑÑ€ÐµÐ´Ð½Ð¸Ð¹ Ð¾Ð±ÑŠÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… Ð´Ð½ÐµÐ¹ + ÑÐµÐ·Ð¾Ð½Ð½Ð¾ÑÑ‚ÑŒ
            recent_volume = train_df['volume'].tail(5).mean()
            day_of_week = test_df['begin'].dt.dayofweek if 'begin' in test_df.columns else 0
            
            if 'begin' in train_df.columns:
                weekday_pattern = train_df.groupby(train_df['begin'].dt.dayofweek)['volume'].mean()
                volume_multipliers = weekday_pattern / weekday_pattern.mean()
                base_volume = recent_volume
                predicted_volumes = []
                for i, date in enumerate(test_df['begin']):
                    weekday = date.dayofweek
                    multiplier = volume_multipliers.get(weekday, 1.0)
                    noise = np.random.normal(0, 0.1)
                    predicted_volumes.append(base_volume * multiplier * (1 + noise))
                
                final_predictions['volume'] = np.array(predicted_volumes)
            else:
                final_predictions['volume'] = np.full(len(test_df), recent_volume)
    
    return final_predictions, return_predictions


train_df = result[:25].copy()
test_df = result[25:].copy()

train_df['begin'] = pd.to_datetime(train_df['begin'])
test_df['begin'] = pd.to_datetime(test_df['begin'])

train_df = train_df.sort_values('begin')
test_df = test_df.sort_values('begin')

final_predictions, return_predictions = optimized_final_pipeline(train_df, test_df)

if final_predictions and 'close' in final_predictions:
    basic_metrics = evaluate_all_targets(test_df, final_predictions)
    positive_predictions = np.sum(return_predictions > 0)
    total_predictions = len(return_predictions)
    
    competition_metrics, corrected_returns, true_returns = evaluate_ensemble_with_metrics(
        test_df, final_predictions, horizon=1
    )
    
    print(f"\nðŸŽ¯ Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ score Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {competition_metrics['Final_Score']:.4f}")

print(final_predictions)
print(return_predictions)
